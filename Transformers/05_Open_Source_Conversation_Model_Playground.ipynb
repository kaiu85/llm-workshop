{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kaiu85/llm-workshop/blob/main/Transformers/05_Open_Source_Conversation_Model_Playground.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Large Language Models at your fingertips\n",
        "\n",
        "This notebook demonstrates how to download the trained weights of a (quite) large language model to this local Google Colab instance, and how encapsulate the model interface into a small function, __alpaca_talk__. This function just takes the trained model, an appropriate tokenizer, and a text string as a prompt as inputs, and outputs a string, which includes the input and the models predicted output. There are no specific tasks here, but feel free to use this function together with code from other notebooks or your private projects. It literally places the power of a large-language model, running on local hardware, at your fingertips."
      ],
      "metadata": {
        "id": "c6ccEueyVdsh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model background\n",
        "\n",
        "This notebook will use a small version (7 billion trainable parameters) of the LLaMA family of models, which was trained by the FAIR team of META on publicly available datasets (in contrast to other large language models, where the proprietary training data still is a matter of concern and public debate). Please take some time to have a proper look at the corresponding Huggingface [model card](https://huggingface.co/decapoda-research/llama-7b-hf), which also provides some basic information on potential biases, risks and harms.\n",
        "\n",
        "For further information on this model family, feel free to also have a look at the corresponding [blog post](https://research.facebook.com/publications/llama-open-and-efficient-foundation-language-models/)."
      ],
      "metadata": {
        "id": "jakYq_8-cJ_q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reminder: Using a large-language model as a coding resource\n",
        "\n",
        "Alternatively, you can go with the flow and try to ask one of the many available large language models to help you. E.g., by copying some code into the model's prompt and asking it to find errors and/or improve your code. Here you could also experiment with different ways of **prompting**, i.e., asking or instructing your model. Usually, by asking the model to first think through a problem sequentially before providing the final answer, you can dramatically improve the performance in more complex reasoning tasks (similar to asking a human to first think through a problem carefully, before trying to provide a definite answer). One very impressive model in this regard is the one by [Perplexity AI](https://www.perplexity.ai/)."
      ],
      "metadata": {
        "id": "fvMu42lnWIG-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lwSKH5kZsLP5"
      },
      "outputs": [],
      "source": [
        "# Install latest bitsandbytes & transformers, accelerate from source\n",
        "!pip install -q -U bitsandbytes\n",
        "!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
        "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
        "!pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
        "# Other requirements for the demo\n",
        "!pip install gradio\n",
        "!pip install sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model.\n",
        "# Note: It can take a while to download LLaMA and add the adapter modules.\n",
        "# You can also use the 13B model by loading in 4bits.\n",
        "\n",
        "import torch\n",
        "from peft import PeftModel    \n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, LlamaTokenizer, StoppingCriteria, StoppingCriteriaList, TextIteratorStreamer\n",
        "\n",
        "model_name = \"decapoda-research/llama-7b-hf\"\n",
        "adapters_name = 'timdettmers/guanaco-7b'\n",
        "\n",
        "print(f\"Starting to load the model {model_name} into memory\")\n",
        "\n",
        "m = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    #load_in_4bit=True,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map={\"\": 0}\n",
        ")\n",
        "m = PeftModel.from_pretrained(m, adapters_name)\n",
        "m = m.merge_and_unload()\n",
        "tok = LlamaTokenizer.from_pretrained(model_name)\n",
        "tok.bos_token_id = 1\n",
        "\n",
        "stop_token_ids = [0]\n",
        "\n",
        "print(f\"Successfully loaded the model {model_name} into memory\")"
      ],
      "metadata": {
        "id": "2QK51MtdsMLu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GenerationConfig\n",
        "\n",
        "def alpaca_talk(text, model, tokenizer):\n",
        "    inputs = tokenizer(\n",
        "        text,\n",
        "        return_tensors=\"pt\",\n",
        "    )\n",
        "    input_ids = inputs[\"input_ids\"].cuda()\n",
        "\n",
        "    generation_config = GenerationConfig(\n",
        "        temperature=0.6,\n",
        "        top_p=0.95,\n",
        "        repetition_penalty=1.2,\n",
        "    )\n",
        "    print(\"Generating...\")\n",
        "    generation_output = model.generate(\n",
        "        input_ids=input_ids,\n",
        "        generation_config=generation_config,\n",
        "        return_dict_in_generate=True,\n",
        "        output_scores=True,\n",
        "        max_new_tokens=1256,\n",
        "    )\n",
        "    output_string = ''\n",
        "    for s in generation_output.sequences:\n",
        "        output_string += tokenizer.decode(s)\n",
        "\n",
        "    return output_string"
      ],
      "metadata": {
        "id": "WhFpQBEo1kyn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"\"\"A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\n",
        "### Human: List mountainbike brands, which are headquartered in California, USA.\n",
        "### Assistant: \"\"\"\n",
        "\n",
        "output_text = alpaca_talk(input_text, m, tok)\n",
        "\n",
        "print(output_text)"
      ],
      "metadata": {
        "id": "Z1A_Fjrp2OPW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_input_text = output_text + '''\n",
        "### Human: Please check your previous answer carefully, fix any errors, and remove all entries from the list, which are not headquartered in CA!\n",
        "### Assistant: '''\n",
        "\n",
        "new_output_text = alpaca_talk(new_input_text, m, tok)\n",
        "\n",
        "print(new_output_text)"
      ],
      "metadata": {
        "id": "WleAqfFE3YRX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vK8Iq9Qo4XEI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}