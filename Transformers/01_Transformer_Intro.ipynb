{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# √úberblick √ºber Transformer und das Huggingface-Repository ü§ó "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Allgemein\n",
    "\n",
    "W√§hrend es Sprachmodelle auf der Basis rekurrenter neuronaler Netze im Prinzip schon seit Jahrzehnten gibt (das [LSTM-Paper](https://direct.mit.edu/neco/article/9/8/1735/6109/Long-Short-Term-Memory) ist von 1997), wurden Transformerarchitekturen, die auf einem sogenannten \"Attention\"-Mechanismen beruhen, erst 2017 vorgestellt. Das erste Paper trug den bezeichnenden Namen [\"Attention is all you need\"](https://arxiv.org/abs/1706.03762).  W√§hrend andere Architekturen, wie Convolutional Neural Nets oder Recurrent Neural Nets, sehr eing√§ngig sind und auch parallelen zu tats√§chlichen neuronalen Netzwerken im visuellen Cortex oder Pr√§frontalcortex aufweisen, muss man f√ºr Transformer etwas umdenken.  Einen sehr guten ersten √úberblick gibt das folgende, f√ºnfmin√ºtige [Video](https://www.youtube.com/watch?v=KN3ZL65Dze0). Eine der besten Einf√ºhrungen in die Details der Transformerarchitektur bietet der folgende [Blogpost](http://jalammar.github.io/illustrated-transformer/).\n",
    "\n",
    "## Pre-Training und Transfer-Learning\n",
    "\n",
    "Neben der neuen Transformerarchitektur war der zweite gro√üe Meilenstein, der zu dem enormen Erfolg der Transformer in der Sprachmodellierung und Textverarbeitung gef√ºhrt hat, der Einsatz von bestimmten so genannten \"Pre-Training Tasks\". Diese Aufgaben lassen ein Transformernetzwerk zun√§chst die statistische Struktur eines gro√üen Datensatzes, z.B. englischer Texte, lernen. Ein klassisches Beispiel f√ºr solch eine Aufgabe f√ºr rekurrente neuronale Netzwerke w√§re z.B. die Vorhersage des n√§chsten Zeichens oder Wortes in einer gegebenen Textsequenz. Solche - auch als \"self-supervised tasks\" bezeichneten - Aufgaben ben√∂tigen nur relativ leicht verf√ºgbare Trainingsdaten, da sie ohne arbeitsaufw√§ndige Annotationen durch Expertinnen auskommen. Das Ziel dieses Pretrainings ist es nicht, dass das Netzwerk direkt auf eine einzelne, spezifische Aufgabe, z.B. Textklassifikation, √úbersetzung, ... trainiert wird, sondern dass das Netzwerk in seinen tieferen Schichten sinnvolle Repr√§sentationen lernt, die man sp√§ter f√ºr viele verschiedene Aufgaben einsetzen kann. Die so vortrainierten Netzwerke kann man dann sp√§ter auf deutlich kleineren, anwendungs- und aufgabenspezifischen Datens√§ten nachtrainieren (\"fine-tunen\"). Dies wird als \"Transfer-Learning\" bezeichnet und senkt die Kosten, die eine einzelne Anwenderin aufbringen muss, um mit solchen Modellen zu arbeiten, drastisch. Denn das initiale Training eines gro√üen Sprachmodelles (oft mit mehreren hundert Millionen Parametern) auf gro√üen Textcorpora (meiste mehrere Millionen Worte) verschlingt [gigantische Rechen-(und damit nat√ºrlich auch Energie-, Zeit- und Geld-)ressourcen](https://arxiv.org/pdf/2004.08900.pdf). Im Gegensatz dazu ben√∂tigt das Nachtrainieren f√ºr spezifische Aufgaben meist nur wesentlich kleinere - daf√ºr aber annotierte - Datens√§tze und kann meist auf einzelnen Workstations innerhalb weniger Stunden bis Tage durchgef√ºhrt werden.\n",
    "\n",
    "## BERT\n",
    "\n",
    "Ein sehr erfolgreiches Beispiel eines so vortrainierten Transformernetzwerkes ist [BERT](https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html). W√§hrend fr√ºhere Transformerarchitekturen noch End-to-End auf spezifische Anwendungen trainiert wurden, wurde hier zum ersten mal mit einer sehr erfolgreichen Kombination aus zwei Pre-Training-Tasks gearbeitet, mit dem Ziel eine abstrakte Repr√§sentation der Texte zu lernen, auf deren Grundlage man verschiedenste Aufgaben l√∂sen kann. Bei der ersten Aufgabe handelt es sich um die \"Masked-Language-Modelling\" Task. Das klingt kompliziert, aber im Prinzip werden dem Netzwerk lediglich l√§ngere Textabschnitte pr√§sentiert, bei denen ein bestimmter Anzahl an Tokens (Textbausteine, je nach Modell k√∂nnen das einzelne Worte, Wortteile oder Buchstaben sein) durch L√ºcken (\"Blanks\") ersetzt wurden, genauso wie fr√ºher bei den L√ºckentexten in der Schule. Ziel des Netzwerkes war es, eine Wahrscheinlichkeitsverteilung zu lernen, die m√∂glichst gut die entfernten Tokens vorhersagt. Diese Task wurde mit einer zweiten Pretraining-Task kombiniert, bei der das Netzwerk zwei S√§tze pr√§sentiert wurden und es entscheiden musste, ob es sich dabei um zwei unabh√§ngig voneinander aus dem Trainingskorpus gezogene S√§tze, oder zwei aufeinanderfolgende S√§tze aus den Trainingscorpus handelt. Die Idee war, das Netzwerk so auch l√§ngere Abh√§ngigkeiten √ºber einzelne S√§tze hinweg lernen musste, um diese Aufgabe l√∂sen zu k√∂nnen. Wenn man nun eine konkrete Aufgabe mittels solcher Transformernetze l√∂sen m√∂chte, startet man nicht mehr bei Null, sondern kann sich f√ºr viele Sprachen z.B. ein vortrainiertes BERT-Modell herunterladen. F√ºr einen guten √úberblick √ºber BERT im speziellen, sehen sie sich das folgende f√ºnfmin√ºtige [Video](https://www.youtube.com/watch?v=zMxvS7hD-Ug) an. Eine detaillierte Einf√ºhrung in Bert gibt dieser [Blogpost](http://jalammar.github.io/illustrated-bert/). Ein konkretes Beispiel, mit dem wir in den n√§chsten Notebooks arbeiten werden, ist ein [deutsches, sogar bereits auf einer deutschen Frage-Antwort-Aufgabe __nachtrainiertes__ BERT-Netzwerk](https://huggingface.co/Sahajtomar/GBERTQnA). Dieses wurde zun√§chst auf einem gro√üen deutschen Textkorpus vortrainiert. Dieser umfasste ca. 163GB Texte aus der deutschen Wikipedia, weiteren Internetquellen, aber auch B√ºcher, Filmuntertitel oder juristische Fachtexte. Die Kosten des Trainings werden nicht angegeben, sollten sich aber mindestens im vierstelligen Bereich bewegen. Dieses [noch aufgabenunabh√§ngige Netzwerk](https://huggingface.co/deepset/gbert-large) wurde von den Entwicklern, einer Kollaboration von deepset.ai und der Bayerischen Staatsbibliothek M√ºnchen, √ºber das Huggingface-Repository (s.u.) frei zur Verf√ºgung gestellt.  Von dort wurde es heruntergeladen und auf einem Frage-Antwort-Datensatz nachtrainiert, der lediglich 5000 Trainingsbeispiele enthielt (je ein Tripel aus 1. einem Informationstext, 2. einer Frage, die anhand dieses Textes beantwortet werden soll, und 3. m√∂glichen korrekten Antworten), und wiederum vom Entwickler, Sahaj Tomar, [frei zur Verf√ºgung gestellt](https://huggingface.co/Sahajtomar/GBERTQnA). Auch hier wurden die Trainingskosten nicht beziffert, aber es ist nicht unrealistisch, so ein finetuning auf einer lokalen Workstation f√ºr einige bis einige hundert Euro durchzuf√ºhren.\n",
    "\n",
    "## Das Huggingface Repository\n",
    "\n",
    "In der Praxis der Sprachverarbeitung mit tiefen neuronalen Netzen spielt momentan das Huggingface-Repository eine gro√üe Rolle. Dieses Repository stellt √ºber das Python-Paket __transformers__ verschiedenste Transformerarchitekturen, die auf verschiedensten Datens√§tzen __vor-__ und zum Teil auch bereits auf konkrete Aufgaben __nachtrainiert__ wurden, sehr einfach zur Verf√ºgung. F√ºr einige, konkrete Anwendungen gibt es sogar bereits vorgefertigte __pipelines__. Diese Pipelines basieren auf Netzwerken, die bereits auf konkreten Aufgaben nachtrainiert wurden und __kapseln__ diese in einfach zu benutzende Funktionen, so dass sich die Anwenderin gar keine Gedanken mehr dar√ºber machen muss, welche Netzwerkarchitektur eigentlich genau im Hintergrund arbeitet, und wie die Inputs und Outputs f√ºr das Netzwerk eventuell vor- bzw. nachverarbeitet werden m√ºssen. Einige dieser Pipelines wollen wir uns in diesem Notebook ansehen. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verschiedene h√§ufige Anwendungen von Transformermodellen\n",
    "\n",
    "Im folgenden werden wir ihnen einige Anwendungsbeispiele zeigen, die die Vielfalt der momentan relativ einfach verf√ºgbaren und einsetzbaren transformerbasierten Sprachmodelle zeigen. Einen gro√üen Anteil an dieser \"Einfachheit\" hat das Huggingface (ü§ó) Repository, das eine Vielzahl von vortrainierten Transformernetzwerken, Datens√§tzen, sowie Funktionen zum einfachen Nachtrainieren oder Integrieren von Transformernetzwerken in eigene Projekte anbietet. Im folgenden werden wir eine kleine Tour durch die verschiedenen \"Pipelines\" von Huggingface machen, die \"off-the-shelf\"-Funktionalit√§t f√ºr verschiedene Aufgaben in Python bereitstellen. F√ºr weitergehende Informationen verweisen wir auf die [entsprechende Seite des Huggingface Repositories](https://huggingface.co/transformers/v3.0.2/task_summary.html).\n",
    "\n",
    "## Aufgabe\n",
    "\n",
    "- __Lesen sie die kurzen Einf√ºhrungstexte f√ºr jede Pipeline, f√ºhren sie dann die dazugeh√∂rige Codezelle(n) aus und achten sie auf die Input- und Outputtexte.__\n",
    "- __Modifizieren__ sie die Inputtexte, die die Pipeline erh√§lt, in den entsprechenden Zellen\n",
    "- Benutzen sie das __Miro-Board, welches in den geteilten Notizen im Hauptraum verlinkt ist__. __Notieren__ sie darin __Inputtexte__ und die entsprechenden __Netzwerk-Outputs__, bei denen die einzelnen Pipelines out-of-the-box:\n",
    "    - __Erstaunlich oder beeindruckend gut__ funktioniert.\n",
    "    - __√úberraschend schlecht__ funktioniert.\n",
    "    - Oder Beispiele, bei denen sie den Output des Netzwerkes auf andere Weise __bemerkenswert oder witzig__ finden.\n",
    "- Sammeln sie in dem __Miro-Board__ f√ºr die einzelnen Beispiele (ihre eigenen aber auch gerne f√ºr die ihrer Kommiliton*innen) __Ideen zu den folgenden Themen__:\n",
    "    - __Woran__ die entsprechende Performance des Netzwerks liegen k√∂nnte.\n",
    "    - Auf __welcher Art von Daten__ man eventuell __nachtrainieren__ k√∂nnte, um die schlechte Performance auf Negativbeispielen zu Verbessern.\n",
    "    - Wie man die __Performance__ eventuell __verbessern__ k√∂nnte, in dem man um die Transformer-Pipeline herum zus√§tzlichen Code schreibt, der die __Netzwerk In- und Outputs vor bzw. nachverarbeitet__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Generation\n",
    "\n",
    "### So einfach ist es mit ü§ó Transformers, einen l√§ngeren Text generieren zu lassen\n",
    "\n",
    "Wundern sie sich beim ausf√ºhren der folgenden Codezellen nicht √ºber die Ladebalken. Sobald sie eine Pipeline zum ersten mal aufrufen, wird zun√§chst das entsprechende vortrainierte Modell einmal vom Huggingface-Repository heruntergeladen. \n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<span style=\"color:black\"><b>Hinweis:</b> Ignorieren sie im folgenden die (so wie diese Box) rot hinterlegten Warnhinweise. Diese werden von der Pipeline automatisch generiert und dienen Entwicklern lediglich zur Fehlersuche.</span>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importiere das \"Pipeline\"-Modul der Huggingface \"transformers\" Bibliothek\n",
    "from transformers import pipeline\n",
    "\n",
    "# Erstelle einen \"Textgenerator\"\n",
    "text_generator = pipeline(\"text-generation\")\n",
    "\n",
    "# Gibt einen Starttext vor (dreifache Anf√ºhrungszeichen erlauben es, \n",
    "# Zeichenketten √ºber mehrere Zeilen zu definieren)\n",
    "prompt = \"\"\"When Abraham Lincoln rose from his grave again during the 2025 Zombie apocalypse, he was very shocked to find out, that \"\"\"\n",
    "\n",
    "# Starte ein vortrainiertex Textmodell und generiere eine vorgegebene Anzahl \n",
    "# an \"Tokens\" (Textbausteine, das k√∂nnen je nach Modell W√∂rter, \n",
    "# Wortteile oder Buchstaben sein. Dar√ºber lernen sie im\n",
    "# n√§chsten Notebook genaueres)\n",
    "generate_length = 200\n",
    "output = text_generator(prompt, max_length=generate_length)\n",
    "\n",
    "# Gibt generierten Text aus\n",
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Masked Language Modelling (a.k.a. \"Fill-the-Blank\")\n",
    "\n",
    "Mit der ü§ó Transformers __fill-mask__ Pipeline ist es m√∂glich, eine Zeichenkette mit __einer__ L√ºcke vorzugeben, die von einem vortrainierten englischen Sprachmodell geschlossen wird.\n",
    "\n",
    "Dazu muss man die L√ºcke innerhalb einer durch __f\"__ und __\"__ eingerahmten, vorgegebenen Zeichenfolge durch __{nlp.tokenizer.mask_token}__ kenntlich machen. Mit den Standardeinstellungen werden dann die f√ºnf komplettierten Zeichenketten ausgegeben, die das trainierte Sprachmodell f√ºr am wahrscheinlichsten h√§lt. Zudem erh√§lt man f√ºr jede komplettierte Zeichenkette einen \"Score\", der beziffert, f√ºr wie wahrscheinlich das trainierte Sprachmodell die entsprechende Vervollst√§ndigung h√§lt (h√∂her bedeutet wahrscheinlicher). Im Prinzip ist es diese Task, die auch f√ºr das Pre-Training von BERT-Modellen Verwendung findet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "nlp = pipeline(\"fill-mask\")\n",
    "\n",
    "# Der Input muss mit f\" beginnen und mit \" enden und\n",
    "# eine \"L√ºcke\" enthalten, die durch {nlp.tokenizer.mask_token} markiert wird.\n",
    "masked_string = f\"Marsians looked very {nlp.tokenizer.mask_token}, when they found out that Elon Musk was going to move to their home.\"\n",
    "\n",
    "output = nlp(masked_string)\n",
    "\n",
    "# Mit den Standardeinstellungen werden 5 vorschl√§ge mit einem jeweiligen Score f√ºr ihre \n",
    "# Wahrscheinlichkeit generiert\n",
    "\n",
    "for i in range(len(output)):\n",
    "    print('Output Nr:')\n",
    "    print(i)\n",
    "    print('Text:')\n",
    "    print(output[i]['sequence'])\n",
    "    print('Score:')\n",
    "    print(output[i]['score'])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarization\n",
    "\n",
    "Die Summarization-Pipeline erlaubt es, __Zusammenfassungen__ vorgegebener Texte zu erzeugen. Man gibt lediglich eine Zeichenkette vor, die den Text enth√§lt, der zusammengefasst werden soll. Dabei erlauben es die dreifachen Anf√ºhrungszeichen __\"\"\" ... \"\"\"__ auch Zeichenketten √ºber mehrere Zeilen zu definieren. Des weiteren kann man noch die minimale und maximale L√§nge der Zusammenfassung in Tokens (Worte, Wortteile oder Buchstaben, je nach Modell) vorgeben. Als Output enth√§lt man eine Zeichenkette, die eine Zusammenfassung des Inputtextes enth√§lt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\"summarization\")\n",
    "\n",
    "ARTICLE = \"\"\" Alan Mathison Turing OBE FRS (/Ààtj ä…ôr…™≈ã/; 23 June 1912 ‚Äì 7 June 1954) was \n",
    "an English mathematician, computer scientist, logician, cryptanalyst, philosopher, \n",
    "and theoretical biologist.[6][7] Turing was highly influential in the development \n",
    "of theoretical computer science, providing a formalisation of the concepts of algorithm \n",
    "and computation with the Turing machine, which can be considered a model of a \n",
    "general-purpose computer.[8][9][10] Turing is widely considered to be the father \n",
    "of theoretical computer science and artificial intelligence.[11]\n",
    "Born in Maida Vale, London, Turing was raised in southern England. He graduated at \n",
    "King's College, Cambridge with a degree in mathematics. Whilst he was a fellow at \n",
    "Cambridge, he published a proof demonstrating that some purely mathematical yes‚Äìno \n",
    "questions can never be answered by computation and defined a Turing machine, and \n",
    "went on to prove the halting problem for Turing machines is undecidable. In 1938, \n",
    "he obtained his PhD from the Department of Mathematics at Princeton University. \n",
    "During the Second World War, Turing worked for the Government Code and Cypher \n",
    "School (GC&CS) at Bletchley Park, Britain's codebreaking centre that produced \n",
    "Ultra intelligence. For a time he led Hut 8, the section that was responsible \n",
    "for German naval cryptanalysis. Here, he devised a number of techniques for \n",
    "speeding the breaking of German ciphers, including improvements to the pre-war \n",
    "Polish bombe method, an electromechanical machine that could find settings for \n",
    "the Enigma machine. Turing played a crucial role in cracking intercepted coded \n",
    "messages that enabled the Allies to defeat the Nazis in many crucial engagements, \n",
    "including the Battle of the Atlantic.[12][13] Due to the problems of counterfactual \n",
    "history, it is hard to estimate the precise effect Ultra intelligence had on the war,\n",
    "[14] but Professor Jack Copeland has estimated that this work shortened the war \n",
    "in Europe by more than two years and saved over 14 million lives.[12]\n",
    "After the war, Turing worked at the National Physical Laboratory, where he designed \n",
    "the Automatic Computing Engine. The Automatic Computing Engine was one of the first \n",
    "designs for a stored-program computer. In 1948, Turing joined Max Newman's Computing \n",
    "Machine Laboratory, at the Victoria University of Manchester, where he helped develop \n",
    "the Manchester computers[15] and became interested in mathematical biology. He wrote \n",
    "a paper on the chemical basis of morphogenesis[1] and predicted oscillating chemical \n",
    "reactions such as the Belousov‚ÄìZhabotinsky reaction, first observed in the 1960s. \n",
    "Turing was prosecuted in 1952 for homosexual acts; the Labouchere Amendment of 1885 \n",
    "had mandated that \"gross indecency\" was a criminal offence in the UK. He accepted \n",
    "chemical castration treatment, with DES, as an alternative to prison. Turing died \n",
    "in 1954, 16 days before his 42nd birthday, from cyanide poisoning. An inquest \n",
    "determined his death as a suicide, but it has been noted that the known evidence \n",
    "is also consistent with accidental poisoning.\n",
    "Despite these accomplishments, he was never fully recognised in his home country \n",
    "during his lifetime because much of his work was covered by the Official Secrets \n",
    "Act. In 2009, following an Internet campaign, British Prime Minister Gordon \n",
    "Brown made an official public apology on behalf of the British government for \n",
    "\"the appalling way he was treated\". Queen Elizabeth II granted Turing a posthumous \n",
    "pardon in 2013. The \"Alan Turing law\" is now an informal term for a 2017 law \n",
    "in the United Kingdom that retroactively pardoned men cautioned or convicted \n",
    "under historical legislation that outlawed homosexual acts.[16] Turing has an \n",
    "extensive legacy with statues of him, many things named after him including an \n",
    "annual award for computer science innovations. He is due to appear on the Bank \n",
    "of England ¬£50 note, to be released in June 2021. A 2019 BBC series, as voted by \n",
    "the audience, named him the greatest person of the 20th century.\"\"\"\n",
    "\n",
    "# maximale L√§nge der Zusammenfassung\n",
    "max_length = 130\n",
    "\n",
    "# minimale L√§nge der Zusammenfassung\n",
    "min_length = 30\n",
    "\n",
    "output = summarizer(ARTICLE, max_length=max_length, min_length=min_length)\n",
    "\n",
    "print(output[0]['summary_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translation\n",
    "\n",
    "Die __translation_en_to_de__ Pipeline erlaubt es, ihrem Namen entsprechend, englische Inputtexte in deutsche Outputtexte zu √ºbersetzen. Es gibt auch weitere Sprachkombinationen (__translation_en_to_fr__, __translation_en_to_ro__), mit denen sie neben der Ver√§nderung des Inputtextes experimentieren k√∂nnen.\n",
    "\n",
    "__Beachten__ sie, dass nur einzelne S√§tze √ºbersetzt werden, d.h. dass die Inputzeichenkette nur bis zum ersten __Punkt \".\"__ verarbeitet wird."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "translator = pipeline(\"translation_en_to_de\")\n",
    "\n",
    "english = \"\"\"The Turing test, originally called the imitation game by Alan Turing in 1950,[2] is a test \n",
    "of a machine's ability to exhibit intelligent behaviour equivalent to, or indistinguishable from, that \n",
    "of a human; to this end, Turing proposed that a human evaluator would judge natural language conversations between a \n",
    "human and a machine.\"\"\"\n",
    "\n",
    "output = translator(english)\n",
    "\n",
    "print('Englischer Text:')\n",
    "print(english)\n",
    "print('')\n",
    "\n",
    "print('Deutsche √úbersetzung:')\n",
    "print(output[0]['translation_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extractive Question Answering (oft verk√ºrzt nur als Question Answering (QA) bezeichnet)\n",
    "\n",
    "In dieser Aufgabe, mit der wir uns auch in den weiteren Notebooks des Workshops besch√§ftigen werden, geht es darum, eine Frage anhand eines bereitgestellten Textes zu beantworten. Dabei ist es die Aufgabe des Netzwerkes, die m√∂gliche Antwort auf die Frage innerhalb des sogenannten \"Kontexts\" zu finden und auszuschneiden (bzw. mit virtuellem Textmarker anzustreichen). D.h. die Ausgabe des Netzwerks besteht aus der wahrscheinlichsten Position einer m√∂glichen Antwort auf die gegebene Frage innerhalb des gegebenen Kontexts. Deshalb wird dieser Aufgabentyp auch als \"Extractive Question Answering\" bezeichnet, da es darum geht die Antwort aus einem vorgegebenen Textabschnitt zu extrahieren. Synonym wird auch die Bezeichnung \"Reading Comprehension\", also Leseverst√§ndnis, f√ºr diesen Aufgabentyp benutzt. \n",
    "\n",
    "__Beachte:__ Die Transformers __question-answering Pipeline__ gibt die entsprechend aus dem Kontext ausgeschnittene Antwort direkt als Zeichenkette aus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "nlp = pipeline(\"question-answering\")\n",
    "\n",
    "# Als \"Kontext\" wird ein Text bezeichnet, aus dem die Antwort auf eine gegebene Frage extrahiert werden soll.\n",
    "# H√§ufig ist die Aufgabe, die Antwort auf eine Frage direkt aus diesem Kontext herauszuschneiden\n",
    "# bzw. mit \"Textmarker\" anzustreichen. Damit ist gemeint, die Position der Antwort innerhalb\n",
    "# des Kontextes auszugeben.\n",
    "\n",
    "# Experimentieren sie mit Unterschiedlichen Textabschnitten und dazu (mehr oder weniger gut)\n",
    "# passenden Fragen.\n",
    "context = \"\"\"Extractive Question Answering is the task of extracting an answer from a text given a question. \n",
    "An example of a question answering dataset is the SQuAD dataset, which is entirely based on that task. \n",
    "If you would like to fine-tune a model on a SQuAD task, you may leverage the \n",
    "examples/question-answering/run_squad.py script from the Huggingface transformer repository.\n",
    "\"\"\"\n",
    "\n",
    "frage1 = \"What is extractive question answering?\"\n",
    "frage2 = \"What is a good example of a question answering dataset?\"\n",
    "\n",
    "# Frage 1 anhand des gegebenen Kontexts beantworten\n",
    "output_frage1 = nlp(question=frage1, context=context)\n",
    "\n",
    "# Antwort auf Frage 1 ausgeben\n",
    "print('Frage: ' + frage1)\n",
    "print('Antwort: ' + output_frage1['answer'])\n",
    "print('Gefunden im Kontext von Position %d bis Position %d!' % (output_frage1['start'], output_frage1['end']))\n",
    "print('Score: %f' % output_frage1['score'])\n",
    "print()\n",
    "\n",
    "# Frage 2 anhand des gegebenen Kontexts beantworten\n",
    "output_frage2 = nlp(question=frage2, context=context)\n",
    "\n",
    "# Antwort auf Frage 2 ausgeben\n",
    "print('Frage: ' + frage2)\n",
    "print('Antwort: ' + output_frage2['answer'])\n",
    "print('Gefunden im Kontext von Position %d bis Position %d!' % (output_frage2['start'], output_frage2['end']))\n",
    "print('Score: %f' % output_frage2['score'])\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversation\n",
    "\n",
    "Eventuell haben sie die __conversation__-Pipeline schon im Rahmen der Chatbot-Notebooks kennen gelernt. Sie bietet eine recht komfortable M√∂glichkeit, ein auf zahlreichen Reddit-Konversationen vortrainiertes Sprachmodell mit wenigen Zeilen Code z.B. in eine eigene Chatbot-Logik einzubinden. Die Pipeline erlaubt es, mit einer Startnachricht (__conversation_start__) eine Konversation mit dem trainierten Transformermodell zu initialisieren und die entsprechende Antwort des Modells (bzw. die gesamte Konversation) auszugeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, Conversation\n",
    "\n",
    "conversational_pipeline = pipeline(\"conversational\")\n",
    "\n",
    "# Initialer Userinput\n",
    "conversation_start = \"Let's watch a movie tonight - any recommendations?\"\n",
    "\n",
    "# Initialisiere Konversation mit der Zeichenkette in\n",
    "# conversation_start (erster Satz der Nutzer*in)\n",
    "conversation = Conversation(conversation_start)\n",
    "\n",
    "# Erzeuge Antwort des Sprachmodells und h√§nge die Antwort an die\n",
    "# bisherige Konversation an\n",
    "conversation = conversational_pipeline(conversation)\n",
    "\n",
    "# Gib kompletten bisherigen Verlauf der Konversation aus\n",
    "print(conversation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In der folgenden Zelle, f√ºgen wir der Konversation den n√§chsten Userinput hinzu und lassen das Modell wieder darauf antworten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Antwort der Nutzer*in auf die bisherige Konversation\n",
    "conversation_next = \"What is it about?\"\n",
    "\n",
    "# F√ºgt Nutzer*innenantwort der Konversation hinzu\n",
    "conversation.add_user_input(conversation_next)\n",
    "\n",
    "# Erzeugt Antwort des Sprachmodells und f√ºgt diese \n",
    "# der laufenden Konversation hinzu\n",
    "conversation = conversational_pipeline(conversation)\n",
    "\n",
    "# Gibt bisherigen Konversationsverlauf aus\n",
    "print(conversation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In dem sie die Zeichenkette der obigen Zelle ver√§ndern und die Zelle immer wieder ausf√ºhren, k√∂nnen sie die Unterhaltung mit dem trainierten Sprachmodell fortsetzen. __Achten__ sie darauf, ob und wann die Konversation eventuell in eine Sackgasse l√§uft und wie sie das (nach __Neustarten der Konversation mittels der ersten Conversation-Codezelle__) eventuell verhindern l√§sst."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wie geht es jetzt weiter?\n",
    "\n",
    "Wie sie hoffentlich selbst erfahren konnten, sind die Huggingface __pipelines__ so gut verpackt, dass man relativ einfach schon recht beeindruckende Ergebnisse erzielen kann, ohne sich √ºberhaupt gro√üe Gedanken √ºber die zu Grunde liegenden Sprachmodelle machen zu m√ºssen. Dies bedeutet jedoch auch, dass man als Anwenderin praktisch keine Kontrolle √ºber die genauen Vor- und Nachverarbeitungsschritte der Inputs und Outputs, sowie √ºber die genutzten Sprachmodelle hat. Aus diesem Grund werden wir uns im n√§chsten Notebook damit besch√§ftigen, wie man eine konkrete Aufgabe (deutsche Fragen anhand vorgegebener, deutscher Texte beantworten) mit Hilfe einer selbst gebauten Pipeline und vortrainierten Transformermodellen l√∂sen kann. Falls sie damit weitermachen m√∂chten, geht es im Notebook *02_Question_Answering_with_BERT.ipynb* weiter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
